{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Парсинг данных с сайта авто.ру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import re\n",
    "from multiprocessing import Process, Lock, Value\n",
    "import json\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500) \n",
    "from tqdm import tqdm\n",
    "from pandas_profiling import ProfileReport\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import timedelta, datetime, date\n",
    "\n",
    "import pandas.api.types as ptypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открываем тестовый датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем список ссылок машин из тестового датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsPageUrl = test['car_url'].to_list()\n",
    "f = open(\"testFirsPageUrl.pkl\",\"wb\")\n",
    "pickle.dump(firsPageUrl,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг по ссылкам проданных машин для получения ссылок похожих машин на проданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = Value('I',0)\n",
    "def getTheSame(url):\n",
    "    sameCars = []\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    div = soup.find(\"div\", class_=\"CardBreadcrumbs\")\n",
    "    try: \n",
    "        with start.get_lock():\n",
    "            start.value+=1\n",
    "            print(start.value)\n",
    "        sameCars.append(div.find_all('a')[-1]['href'])\n",
    "        print(url, 'parsed')\n",
    "        return sameCars\n",
    "    except AttributeError:  \n",
    "        print(\"No CardBreadcrumbs call found\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем мультипроцессинг для ускоренного парсинга\n",
    "pool = ThreadPool()\n",
    "result = pool.map(getTheSame, firsPageUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформируем полученый список списков в плоский список\n",
    "flat_results = []\n",
    "for i in result:\n",
    "    if i != None:\n",
    "        flat_results.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34451\n",
      "5842\n"
     ]
    }
   ],
   "source": [
    "# Проверяем колличество уникальных линков в списке. У нас 5842 разновидности машин\n",
    "print(len(flat_results))\n",
    "flat_results = list(set(flat_results))\n",
    "results = flat_results\n",
    "print(len(flat_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем полученый список локально\n",
    "with open(\"sameCarsURLs.txt\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем список из локального файла\n",
    "with open(\"sameCarsURLs.txt\", \"rb\") as fp:  \n",
    "    sameCarsURLs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5842"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sameCarsURLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй парсинг - получение ссылок на похожие машины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carsInList(url1):\n",
    "    carsURLs = []\n",
    "    r = requests.get(url1)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    div = soup.select('.ListingItem-module__main .ListingItem-module__thumb a')\n",
    "    with start.get_lock():\n",
    "            start.value+=1\n",
    "            print(start.value)\n",
    "    for i in div:\n",
    "        carsURLs.append(i['href'])\n",
    "    return carsURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем мультипроцессиг\n",
    "start = Value('I',0)\n",
    "pool = ThreadPool()\n",
    "results = pool.map(carsInList, sameCarsURLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Сохраняем полученый список локально\n",
    "with open(\"sameCarsURLsSecond.txt\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68890"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# делаем список плоским и проверяем наличие уникальных значений\n",
    "flat_results_2 = []\n",
    "for i in results:\n",
    "    for a in i:\n",
    "        flat_results_2.append(a)\n",
    "len(set(flat_results_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск машин по Москве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список брендов\n",
    "brand = list(test.brand.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_url(brand):\n",
    "    carsURLs = [] # здесь соберем список списков(эл-т - список параметров конкретного автомобиля)\n",
    "    for i in range(1, 94):     # кол-во страниц для парсинга с авто.ру   \n",
    "        response = requests.get(f'https://auto.ru/moskva/cars/{brand}/used/?output_type=list&page={i}')\n",
    "        print(f'{brand} page={i}')\n",
    "        with start.get_lock():\n",
    "            start.value+=1\n",
    "            print(start.value)\n",
    "        if response.status_code != 200:\n",
    "            raise BaseException(\"response code\" + str(response.status_code))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        div = soup.select('.ListingItem-module__main .ListingItem-module__thumb a')\n",
    "            \n",
    "        for j in div:\n",
    "            carsURLs.append(j['href'])\n",
    "    return carsURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOLVO page=99\n",
      "1\n",
      "AUDI page=99\n",
      "2\n",
      "INFINITI page=99\n",
      "3\n",
      "HONDA page=99\n",
      "4NISSAN page=99SKODA page=99\n",
      "\n",
      "\n",
      "5\n",
      "6\n",
      "MERCEDES page=99\n",
      "7\n",
      "BMW page=99\n",
      "8\n",
      "VOLVO page=100\n",
      "9\n",
      "HONDA page=100\n",
      "10\n",
      "SKODA page=100\n",
      "11\n",
      "NISSAN page=100\n",
      "12\n",
      "INFINITI page=100\n",
      "13\n",
      "AUDI page=100\n",
      "14\n",
      "MERCEDES page=100\n",
      "15\n",
      "BMW page=100\n",
      "16\n",
      "VOLVO page=101\n",
      "17\n",
      "HONDA page=101\n",
      "18\n",
      "MERCEDES page=101\n",
      "19\n",
      "AUDI page=101\n",
      "20\n",
      "SKODA page=101\n",
      "21\n",
      "VOLVO page=102\n",
      "22\n",
      "INFINITI page=101\n",
      "23\n",
      "NISSAN page=101\n",
      "24\n",
      "BMW page=101\n",
      "25\n",
      "HONDA page=102\n",
      "26\n",
      "MERCEDES page=102\n",
      "27\n",
      "VOLVO page=103\n",
      "28\n",
      "SKODA page=102\n",
      "29\n",
      "AUDI page=102\n",
      "30\n",
      "NISSAN page=102\n",
      "31\n",
      "INFINITI page=102\n",
      "32\n",
      "HONDA page=103\n",
      "33\n",
      "BMW page=102\n",
      "34\n",
      "VOLVO page=104\n",
      "35\n",
      "MERCEDES page=103\n",
      "36\n",
      "SKODA page=103\n",
      "37\n",
      "AUDI page=103\n",
      "38\n",
      "NISSAN page=103\n",
      "39\n",
      "HONDA page=104\n",
      "40\n",
      "BMW page=103\n",
      "41\n",
      "INFINITI page=103\n",
      "42\n",
      "MERCEDES page=104\n",
      "43\n",
      "VOLVO page=105\n",
      "44\n",
      "HONDA page=105\n",
      "45\n",
      "SKODA page=104\n",
      "46\n",
      "NISSAN page=104\n",
      "47\n",
      "AUDI page=104\n",
      "48\n",
      "BMW page=104\n",
      "49\n",
      "MERCEDES page=105\n",
      "50\n",
      "INFINITI page=104\n",
      "51\n",
      "VOLVO page=106\n",
      "52\n",
      "HONDA page=106\n",
      "53\n",
      "NISSAN page=105\n",
      "54\n",
      "SKODA page=105\n",
      "55\n",
      "AUDI page=105\n",
      "56\n",
      "BMW page=105\n",
      "57\n",
      "MERCEDES page=106\n",
      "58\n",
      "INFINITI page=105\n",
      "59\n",
      "VOLVO page=107\n",
      "60\n",
      "HONDA page=107\n",
      "61\n",
      "SKODA page=106\n",
      "62\n",
      "AUDI page=106\n",
      "63\n",
      "NISSAN page=106\n",
      "64\n",
      "MERCEDES page=107\n",
      "65\n",
      "BMW page=106\n",
      "66\n",
      "INFINITI page=106\n",
      "67\n",
      "VOLVO page=108\n",
      "68\n",
      "HONDA page=108\n",
      "69\n",
      "SKODA page=107\n",
      "70\n",
      "AUDI page=107\n",
      "71\n",
      "NISSAN page=107\n",
      "72\n",
      "BMW page=107\n",
      "73\n",
      "MERCEDES page=108\n",
      "74\n",
      "SKODA page=108\n",
      "75\n",
      "VOLVO page=109\n",
      "76\n",
      "HONDA page=109\n",
      "77\n",
      "INFINITI page=107\n",
      "78\n",
      "AUDI page=108\n",
      "79\n",
      "NISSAN page=108\n",
      "80\n",
      "SKODA page=109\n",
      "81\n",
      "BMW page=108\n",
      "82\n",
      "MERCEDES page=109\n",
      "83\n",
      "LEXUS page=99\n",
      "84\n",
      "TOYOTA page=99\n",
      "85\n",
      "INFINITI page=108\n",
      "86\n",
      "MITSUBISHI page=99\n",
      "87\n",
      "AUDI page=109\n",
      "88\n",
      "VOLKSWAGEN page=99\n",
      "89\n",
      "LEXUS page=100\n",
      "90\n",
      "BMW page=109\n",
      "91\n",
      "TOYOTA page=100\n",
      "92\n",
      "MITSUBISHI page=100\n",
      "93\n",
      "INFINITI page=109\n",
      "94\n",
      "NISSAN page=109\n",
      "95\n",
      "VOLKSWAGEN page=100\n",
      "96\n",
      "LEXUS page=101\n",
      "97\n",
      "TOYOTA page=101\n",
      "98\n",
      "MITSUBISHI page=101\n",
      "99\n",
      "VOLKSWAGEN page=101\n",
      "100\n",
      "LEXUS page=102\n",
      "101\n",
      "TOYOTA page=102\n",
      "102\n",
      "MITSUBISHI page=102\n",
      "103\n",
      "VOLKSWAGEN page=102\n",
      "104\n",
      "LEXUS page=103\n",
      "105\n",
      "TOYOTA page=103\n",
      "106\n",
      "VOLKSWAGEN page=103\n",
      "107\n",
      "MITSUBISHI page=103\n",
      "108\n",
      "LEXUS page=104\n",
      "109\n",
      "TOYOTA page=104\n",
      "110\n",
      "VOLKSWAGEN page=104\n",
      "111\n",
      "MITSUBISHI page=104\n",
      "112\n",
      "LEXUS page=105\n",
      "113\n",
      "TOYOTA page=105\n",
      "114\n",
      "MITSUBISHI page=105\n",
      "115\n",
      "VOLKSWAGEN page=105\n",
      "116\n",
      "TOYOTA page=106\n",
      "117\n",
      "LEXUS page=106\n",
      "118\n",
      "MITSUBISHI page=106\n",
      "119\n",
      "VOLKSWAGEN page=106\n",
      "120\n",
      "TOYOTA page=107\n",
      "121\n",
      "MITSUBISHI page=107\n",
      "122\n",
      "LEXUS page=107\n",
      "123\n",
      "VOLKSWAGEN page=107\n",
      "124\n",
      "LEXUS page=108\n",
      "125\n",
      "TOYOTA page=108\n",
      "126\n",
      "MITSUBISHI page=108\n",
      "127\n",
      "VOLKSWAGEN page=108\n",
      "128\n",
      "TOYOTA page=109\n",
      "129\n",
      "LEXUS page=109\n",
      "130\n",
      "VOLKSWAGEN page=109\n",
      "131\n",
      "MITSUBISHI page=109\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Запескаем мультипроцессинг\n",
    "\n",
    "start = Value('I',0)\n",
    "pool = ThreadPool()\n",
    "moskowCars = pool.map(parsing_url, brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем плоский список\n",
    "\n",
    "flat_results = []\n",
    "for i in moskowCars:\n",
    "    for j in i:\n",
    "        flat_results.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"allMoscowCarsURLsFrom95.txt\", \"wb\") as fp:\n",
    "    pickle.dump(flat_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"allMoscowCarsURLs.txt\", \"rb\") as fp:  \n",
    "    mskCarsURLs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28179\n",
      "28074\n"
     ]
    }
   ],
   "source": [
    "# Проверяем длину списока машин проланых в Москве\n",
    "print(len(mskCarsURLs))\n",
    "print(len(list(set(mskCarsURLs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Третий парсинг машин и создание датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [6:10:32<00:00,  1.31s/it]  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame() \n",
    "\n",
    "for url in tqdm(flat_results_2[]):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        script1 = soup.find('script', type='application/ld+json').contents[0]\n",
    "        script1_json = json.loads(script1)\n",
    "        script2 = soup.find('script', id='initial-state').contents[0]\n",
    "        script2_json = json.loads(script2)\n",
    "        ul = soup.find('ul', class_='CardInfo')\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    train = {}\n",
    "\n",
    "    try:\n",
    "        train['bodyType'] = script1_json['bodyType']   \n",
    "    except:\n",
    "        np.nan        \n",
    "    try:\n",
    "        train['brand'] = script1_json['brand']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['car_url'] = script1_json['offers']['url']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['color'] = script1_json['color']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['complectation_dict'] = np.nan\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['description'] = script1_json['description']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['engineDisplacement'] = script1_json['vehicleEngine']['engineDisplacement']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['enginePower'] = script1_json['vehicleEngine']['enginePower']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['equipment_dict'] = str(script2_json['card']['vehicle_info']['equipment'])\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['fuelType'] = script1_json['vehicleEngine']['fuelType']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['image'] = script1_json['image']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['mileage'] = script2_json['card']['state']['mileage']\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['modelDate'] = script1_json['modelDate']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['model_info'] = str(script2_json['card']['vehicle_info']['model_info'])\n",
    "    except:\n",
    "        np.nan   \n",
    "    try:\n",
    "        train['model_name'] = script2_json['card']['vehicle_info']['model_info']['code']\n",
    "    except:\n",
    "        np.nan\n",
    "        \n",
    "    try:\n",
    "        train['name'] = script2_json['card']['vehicle_info']['tech_param']['human_name']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['numberOfDoors'] = script1_json['numberOfDoors']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['parsing_unixtime'] = int(time.time())\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['priceCurrency'] = script1_json['offers']['priceCurrency']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['productionDate'] = script1_json['productionDate']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['sell_id'] = script2_json['card']['id']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['super_gen'] = str(script2_json['card']['vehicle_info']['tech_param'])\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['vehicleConfiguration'] = script1_json['vehicleConfiguration']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['vehicleTransmission'] = script1_json['vehicleTransmission']\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['vendor'] = np.nan\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['Владельцы'] = ul.find_all('li', class_='CardInfoRow_ownersCount')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['Владение'] = ul.find_all('li', class_='CardInfoRow_owningTime')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['ПТС'] = ul.find_all('li', class_='CardInfoRow_pts')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['Привод'] = ul.find_all('li', class_='CardInfoRow_drive')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['Руль'] = ul.find_all('li', class_='CardInfoRow_wheel')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan    \n",
    "    try:\n",
    "        train['Состояние'] = ul.find_all('li', class_='CardInfoRow_state')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan\n",
    "    try:\n",
    "        train['Таможня'] = ul.find_all('li', class_='CardInfoRow_customs')[0].find_all('span')[1].text\n",
    "    except:\n",
    "        np.nan\n",
    "    try:\n",
    "        train['price'] = script1_json['offers']['price']\n",
    "    except:\n",
    "        np.nan \n",
    "        \n",
    "    \n",
    "    \n",
    "    df_train = df_train.append([train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train1.csv')\n",
    "df_train2 = pd.read_csv('df_train2.csv')\n",
    "df_train3 = pd.read_csv('df_train3.csv')\n",
    "df_train4 = pd.read_csv('df_train4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_train2,df_train3, df_train4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финального датафрейма локально\n",
    "\n",
    "df.to_csv('df_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
